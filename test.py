# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18tQoOdmmyjp9IJVzEglbLNVij8eCqoEd
"""

# –ü—ñ–¥–∫–ª—é—á–∞—î–º–æ Google Drive
from google.colab import drive
drive.mount('/content/drive')
from pathlib import Path

# üîÅ –í–∫–∞–∂–∏ —Å–≤—ñ–π —Ä–µ–∞–ª—å–Ω–∏–π —à–ª—è—Ö –¥–æ –ø–∞–ø–∫–∏ –Ω–∞ Google –î–∏—Å–∫—É
# –ù–∞–ø—Ä–∏–∫–ª–∞–¥, —è–∫—â–æ –ø–æ–≤–Ω–∏–π —à–ª—è—Ö: /MyDrive/test/dataset
folder_path = Path("/content/drive/MyDrive/test/dataset/mobs")

# –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ .jpg —Ñ–∞–π–ª—ñ–≤ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
image_count = len(list(folder_path.rglob("*.jpg")))
print(f"üñºÔ∏è –ö—ñ–ª—å–∫—ñ—Å—Ç—å .jpg –∑–æ–±—Ä–∞–∂–µ–Ω—å: {image_count}")

from google.colab import drive
drive.mount('/content/drive')
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf
from keras.layers import Rescaling

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

dataset_dir = Path("/content/drive/MyDrive/test/dataset/mobs")
batch_size = 16
img_width = 256
img_height = 256

train_ds = tf.keras.utils.image_dataset_from_directory(
    dataset_dir,
    validation_split = 0.2,
    subset = "training",
    seed = 123,
    image_size = (img_height, img_width),
    batch_size = batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
    dataset_dir,
    validation_split = 0.2,
    subset = "validation",
    seed = 123,
    image_size = (img_height, img_width),
    batch_size = batch_size)

class_names = train_ds.class_names
print(f"Class names: {class_names} ")

#cache
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

#create model
num_classes = len(class_names)
model = Sequential([
  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),

  #augementation
  layers.RandomFlip("horizontal",
                    input_shape=(img_height,
                                 img_width,
                                 3)),
  layers.RandomRotation(0.1),
  layers.RandomZoom(0.1),
  layers.RandomContrast(0.2),

  #regulization
  layers.Dropout(0.2),

  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),

  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),

  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),

  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])

#compile model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.summary()



#train model
epochs=15
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

# visualize training and validation results
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()


model.save_weights('/content/drive/MyDrive/test/yolo/mobs.weights.h5')
print("Model saved")

import tensorflow as tf
import numpy as np
from tensorflow.keras.utils import load_img, img_to_array
from PIL import Image

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ
img_height = 256
img_width = 256

# –®–ª—è—Ö –¥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
image_path = "/content/drive/MyDrive/test/yolo/second.png"

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
img = load_img(image_path, target_size=(img_height, img_width))
img_array = img_to_array(img)
img_array = tf.expand_dims(img_array, 0)  # [1, height, width, 3]

# –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

# –í–∏–≤—ñ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
print("–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ {} ({:.2f}% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å)".format(
    class_names[np.argmax(score)],
    100 * np.max(score)))

# –ü–æ–∫–∞–∑ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
img = Image.open(image_path)
img.show()

# 1. –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è Google Drive
from google.colab import drive
drive.mount('/content/drive')
# 2. –Ü–º–ø–æ—Ä—Ç–∏
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array
import numpy as np
import matplotlib.pyplot as plt
# 3. –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
dataset_dir = "/content/drive/MyDrive/test/dataset"  # —Ç—É—Ç –ø–æ–≤–∏–Ω–Ω—ñ –±—É—Ç–∏ –ø–∞–ø–∫–∏ mobs/ —ñ empty/
img_height = 256
img_width = 256
batch_size = 16
# 4. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É (–±—ñ–Ω–∞—Ä–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è)
train_ds = image_dataset_from_directory(
    dataset_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    label_mode='binary'
)

val_ds = image_dataset_from_directory(
    dataset_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    label_mode='binary'
)
# 5. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è pipeline
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
# 6. –ü–æ–±—É–¥–æ–≤–∞ –º–æ–¥–µ–ª—ñ
model = models.Sequential([
    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),

    layers.Conv2D(16, 3, activation='relu'),
    layers.MaxPooling2D(),

    layers.Conv2D(32, 3, activation='relu'),
    layers.MaxPooling2D(),

    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(),

    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # –æ–¥–∏–Ω –Ω–µ–π—Ä–æ–Ω –¥–ª—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ
])

# 7. –ö–æ–º–ø—ñ–ª—è—Ü—ñ—è –º–æ–¥–µ–ª—ñ
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)
# 8. –ù–∞–≤—á–∞–Ω–Ω—è
epochs = 15
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs
)
# 9. –ì—Ä–∞—Ñ—ñ–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç—ñ
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Accuracy over Epochs')
plt.show()
# 10. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤–∞–≥ —É Google Drive
model.save_weights('/content/drive/MyDrive/test/yolo/mob_classifier.weights.h5')

# —à–ª—è—Ö –¥–æ —Å–∫—Ä—ñ–Ω–∞
image_path = "/content/drive/MyDrive/test/yolo/Shot3.jpg"

# –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞
img = load_img(image_path, target_size=(img_height, img_width))
img_array = img_to_array(img)
img_array = tf.expand_dims(img_array, 0)

# –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
prediction = model.predict(img_array)
prob = prediction[0][0]

# —Ä–µ–∑—É–ª—å—Ç–∞—Ç
if prob > 0.5:
    print(f"‚úÖ –ù–∞ —Å–∫—Ä—ñ–Ω—à–æ—Ç—ñ —î –º–æ–±! ({prob:.2f})")
else:
    print(f"‚ùå –ú–æ–± –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. ({prob:.2f})")

# –ø–æ–∫–∞–∑ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
plt.imshow(img)
plt.axis('off')  # –ø—Ä–∏–±—Ä–∞—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–Ω—É —Å—ñ—Ç–∫—É
plt.title("–°–∫—Ä—ñ–Ω—à–æ—Ç –¥–ª—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è")
plt.show()